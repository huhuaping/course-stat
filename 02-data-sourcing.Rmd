---
output:
  xaringan::moon_reader:
    seal: false
    lib_dir: libs
    css: 
      - default
      - default-fonts
      - duke-blue
      - hygge-duke
      - libs/cc-fonts.css
      - libs/figure-captions.css
      - libs/mycss/my-custom.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"

---
background-image: url("pic/slide-front-page.jpg")
class: center,middle

# 统计学原理(Statistic)

<!---    chakra: libs/remark-latest.min.js --->

### 胡华平

### 西北农林科技大学

### 经济管理学院数量经济教研室

### huhuaping01@hotmail.com

### `r Sys.Date()`

```{r global_options, echo=F,message=FALSE,warning=F}
source("R/set-global.R")

```


```{r ex-math-eq}
source("R/external-math-equation.R")
```

<style type="text/css">
.remark-slide-content {
    font-size: 24px;
    padding: 1em 4em 1em 4em;
}
</style>

---
class: inverse, center, middle
# 第二章

## 数据收集、整理和清洗

---
layout: false
class: inverse, center, middle, duke-softblue

# 2.1 数据来源与形式

---
layout: true
  
<div class="my-header"></div>

<div class="my-footer"><span>huhuaping@   第02章 数据收集、整理和清洗   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
2.1 数据来源与形式</span></div> 

---

## 数据来源

不同研究方法会产生不同类型数据：

- 观察数据

- 调查数据

- 实验数据


---

## 数据来源

从产生数据的方式方法上又可以有：

- 问卷数据

- 访谈数据

- 文献数据

- 痕迹数据：大数据。（注意不是**痕迹证据**！）

在获得数据的同时，  应该还有一份数据，是记录数据获得过程的，通常称之为日志，  它要记录数据是从哪里来的、什么情况下得到的、数据的基本特征又是什么，  比如文字数据有多少页、图片数据有多少张，这就是日志数据


---

## 数据载体和形态


从是否数字化来看：

- 数字化的数据

- 非数字化的数据


从是否数值化来看：

- 数值数据

- 非数值数据

---

## 数据载体和形态

从具体形态来看：

- 文本数据：
    
    - 访问、观察中的文字记录
    - 数字化的字符形态的数据
    - 任何文字加载体的数据，比如文字加载于纸张、羊皮卷等

- 图片数据：

    - 访谈时拍的照片、搜集到的图片、照片的底片等等
    - 数字化为像素点形态的图片数据
    - 任何图形加载体的数据，比如图形加载于纸张、胶片、计算机存储等

---

## 数据载体和形态

- 音频数据：
    - 访问录音、观察中的语音日志、搜集到的音频记录等。 
    - 数字化为波形形态的音频数据。
    - 任何音频加上载体，比如音频加载于钢丝、胶片、磁带、光碟、磁碟、闪存盘、硬盘等

- 视频数据：
    - 访谈时的全程录像、搜集到的各种各样视频。
    - 数字化为像素点加上波形形态的视频数据
    - 视频加上载体，比如比如视频加载于胶片、光碟、闪存盘、硬盘等
    
- 实物数据

    - 任何有实物才可以完整保存信息的实物载体数据
    - 访谈中搜集到的实物、观察中观察到的实物，比如出土文物、建筑等

???

数据的类型主要依据来源和载体形态有不同的划。

对数据整理而言，载体形态是最基本的分类，不同载体形态的整理方式会有不同。 

---

### 课堂思考


以上关于数据来源与形式的分类是完全是互斥的吗？

以调查问卷为例：

- 传统纸版问卷，主要是文字、图片形态的数据。

- 新媒体电子问卷，不管是哪一个类型的电子问卷，主要是数据形态的数据，当然也会有图片的、音频的、视频的数据。 


以上的分类并不完全是互斥的，只是根据显性的特征来做一些划分，其实我们很难找到一个标准把数据的形态类型区分得非常清楚。 

???

有同学可能会说：“老师，问卷上不是有数吗？”数在数字化里有两个定义的，一个是字符型的，一个是数值型的，有不同的含义；在纸版上不管是数还是字，都是文字形，在纸版问卷中，除了数还有图片，或者是图画，只有这两个形态的数据。  


---

### 课堂思考

数字与数值是一个意思吗？

图片、音频、视频看起来的确是数字的，但数字不等于数值！

- 传统照片不是数字的。

- 数码照片的数字指的是像素点的数字

- 音频、视频是同样的道理。


---

### 课堂思考

>“老师，不管什么时候我都要用计算机做笔记的。” 

信息化时代，传统手写记录的文本数据是不是越来越没有价值？

- 用计算机或各类终端设备来做电子化记录。

- 用笔和本子做传统记录。 

---
layout: false
class: inverse, center, middle, duke-softblue

# 2.2 数据收集

---
layout: true
  
<div class="my-header"></div>

<div class="my-footer"><span>huhuaping@   第02章 数据收集、整理和清洗   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
2.2 数据收集</span></div> 

---

## 收集二手数据


---

## 收集调查数据



---

## 收集实验数据

---
layout: false
class: inverse, center, middle, duke-softblue

# 2.3 数据整理

---
layout: true
  
<div class="my-header"></div>

<div class="my-footer"><span>huhuaping@   第02章 数据收集、整理和清洗   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
2.2 数据整理</span></div> 


---

## 数据整理的工作流程

在数据收集过程中，重要的是条分缕析。让数据的相关信息清清楚楚，用起来方便，不至于混乱。 

- 分类存储

    - 依据数据的载体类型、研究的时间需来进行分类，并且把分类好的数据，无论是什么载体形式的数据，采用合适的存放工具进行存放。  
    - 比如，纸板问卷，不能随便堆放，需要按照一定的分类标准进行存放，便于后续的工作。比如，录入，质量评估，复查等等。  

- 建立目录

    - 存放的目的不仅仅只为了存储，更重要的是为了便于使用，建立目录就是便于利用的方式之一。 
    - 目录是用于检索的，对调查获得数据建立目录，也是为了方便检索。  

- 编制索引

    - 对于复杂数据，还需要在目录与存储之间建立关联，这就是索引

---

## 收集整理的工作记录

数据收集和整理中不仅需要核实，还需要记录，做笔记，把搜集数据、整理的工作信息记录整理清楚。主要记录：

--

.pull-left[

- 数据来源信息：

    - 如调查项目，调查人， 采集人，采集时间，地点，对象。

- 数据载体类型信息：

    - 具体是什么载体？ 比如，纸张的、数字的。  

- 数据描述信息：关于数据基本状态的描述

    - 有多大规模，什么内容，关联什么主题，等等。 

]

--
    
.pull-right[

- 数据分类信息：

    - 无论是按照载体形态分类还是按照其他标准分类，一个大型项目需要对原始数据根据数据使用，建立基本分类。  
    
- 数据存储信息：

    - 数据以什么样的载体，什么样的方式存储在什么位置？ 
    - 与数据安全相关的信息，如存储的版本、份数、时间变化关系等。  
]

---

## 数据化检索和数据安全


>“老师，能把上星期发给我的课件再发一遍吗？我忘记放到哪了？”

>“老师，非常的崩溃！电脑的硬盘坏了，写的东西都没有了！”

上述记录信息要尽可能的保存若干个版本。

- 纸和笔的传统版本。便于在需要的时候翻阅，尤其是使用范围相对较广的数据。 

- 数字化可检索的版本。为什么要做数字化的可检索版本？

    - **目录树法**（相对简单的数据）
    
    - 建立专门的**数据库**（针对异常复杂或庞大的数据）


---

## 数据化检索和数据安全

数字化数据有一些需要特别注意的问题：

--

- 数据存储。随时都有若干个备份！

--
    - 数字化的数据从最初的纸袋到今天的磁盘、硬盘，有各种介质。由于介质的可靠性不同，数据的安全性也不相同。
    - 美国的“911”事件。美国联储会的主席格林斯潘知道这个消息的第一时间，他担心的  不是“911”的伤亡情况，而是美国金融数据的安全。

--

- 数据安全。安全的风险，要么来自于使用者的误操作，要么来自于内部或者外部的有意攻击。

--

    - 离线保存的目的不仅仅是为了应对各种预想不到的不测，更重要的是为了防止数据泄露。
    - **斯诺登事件**：任何在线数据事实上都是不安全的，都有安全隐患。

---

## 数据化检索和数据安全

**文本数据**的安全：

- 文本数据的安全威胁主要来自于不可抗力的一些因素，比如说自然灾害、风蚀等。

- 当然也来源于人为因素。比如说错误的识别，本来是很重要的数据，却被当作了废纸。


**非数字化数据**的安全：

- 图片数据的载体形态比较复杂，胶片、图片由于介质存储特征的差异，不可以混合放置而保管。如胶片就需要防潮，通常要使用防潮器皿。 

- 实物数据的安全具有独特性，应根据实物实物特征进行科学整理和安全管理。比如说兵马俑，那就在兵马俑的原址上盖一个博物馆进行整理。


---
layout: false
class: inverse, center, middle, duke-softblue

# 2.4 数据清洗

---
layout: true
  
<div class="my-header"></div>

<div class="my-footer"><span>huhuaping@   第02章 数据收集、整理和清洗   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
2.4 数据清洗</span></div> 

---

## 数据清洗工作的内容

**数据整理**主要是分类和梳理，**数据清洗**主要讨论的则是检错。通过这两部分的工作减少人为的误差，降低调查误差。数据清洗包括四个工作内容：

- **真实性评估**：确认数据是真实的，不是道听途说，不是张冠李戴，更不是杜撰臆想。
    
    - “假新闻”现象就是调查数据在真实性层面出现的问题。
    - 微信群里“令人发指”的各类长辈转发

- **完整性评估**：数据应与研究工作的目标要求相符，研究不需要的就不应该出现在数据中，研究需要的在数据中就不应该缺少。 

    - 如果需要补值，就应继续补充收集数据。

---

## 数据清洗工作的内容

- **可用性评估**：数据是不是可以用于数据库化了？如果不能，还需要做怎样的数据加工？

    - 比如对图片数据、 音频、视频数据，甚至文本数据是不是还要做数字化工作。
    - 对于**痕迹数据**，尤其是大数据，如果不是直接采用大数据分析，而是应用于单机分析或服务器分析，是不是还要根据数据量进行抽样。
    - **脱敏化**处理。对有可能泄露受访者隐私、泄露传感器使用者隐私的部分，还需要做匿名化工作。 

- **错误性评估**：评估数据可能的错误来源、可能的错误大小，及其对数据质量的影响。 

---

## 数据清洗工作的内容

以**调查问卷数据**的清洗为例：

- 真实性的清洗：要确认数据来自于受访者。 

- 完整性的清洗：主要看样本无应答，也就是一整份问卷没有应答。以及选项无应答，也就是应该应答的访题没有应答。 

- 可用性的清洗：主要是看编码是否完成，权数是否可行，以及缺失值如何标记和处理。

- 错误性的清洗：主要是清洗调查环节的错误，比如样本错误、应答人错误、应答方式错误。


---

## 数据清洗工作的记录

清洗数据工作中的每一项活动都要有记录。记录信息包括：

- 清洗工作的信息记录： 

    -数据清洗每一个步骤的做法、参与人、时间、地点、过程信息。 

- 与清洗内容相关联的信息记录：

    - 数据真实性信息。比如是否真实？是否存在编造、作弊嫌疑？哪些部分存在不真实？ 怎么样不真实？等等。
    - 数据完整性信息。比如是否完整？是否有缺失？如果有缺失，哪些部分缺失？缺失哪些数据？等等。
    - 数据可用性信息。比如问卷数据是否加权？录音数据是否转录为文字？痕迹数据是否数据化了？大数据如何处理？是运用云计算策略，还是裁剪为单机计算容量？等等。
    - 数据错误性信息。比如问卷数据中的缺失，文献数据中的差错，以及其他可能导致误差的差错等等。 


---

## 数据清洗记录的备份与安全

数据清洗的记录信息应尽可能地保留若干不同的版本。一般包括纸笔版本和数字化版本。纸笔版本便于随时翻阅，数字化版本，便于交流，也便于检索。 

- **笔记的清洗**。不管是哪一类的笔记，所有的笔记都有私用和公用之别，通常人们做笔记都是做给自己看的（**私用笔记**）。

    - 你把自己的笔记给别人看，别人能看懂吗？
    - 在正式使用之前，需要把笔记数据通过清洗，变成任何使用者都可读的笔记（**公用笔记**），
    - 这就是格式化问题，就是把你个人的笔记清洗为数据笔记。 

---

## 数据清洗记录的备份与安全
 
- **对音频要抄录**：

    - 把语音文档，不管是磁带录音，还是数字录音，抄录为文字，表述为文字或者文字加图片这样的格式。
    - 数字录音还有一个**格式清洗**问题，不同数字设备的录音，可能会 采用不同的格式。
    - 比如olympus的早期设备，采用的就是它自己的格式，DSS格式；如果不是采用它自己的软件就读不出来，最好呢，是转化为通用的格式，比如mp3格式。 
 
- **对视频清洗编码**：
    - 如果是非数字录像，最好先转化为数字格式
    - 如果已经是数字录像，对视频清洗编码需要给出**时间记录码**。

---

## 数据清洗工作的几点忠告

> 哦，已经数字化了，可以扔了，那个没用了，可以扔了。

- 不要轻易地丢弃任何一段看起来没有用处的信息，信息载体。

- 清洗不是仍东西，是清洗数据，让数据清晰化。

- 清洗的目的就是将特异性的数据，转化为公共性的数据、分析研究者都可以读的数据。

- 在清洗的过程中，千万要保留原始观察记录。

    - 一般而言，原始问卷至少要保留十年以上，访谈记录和观察笔记一般要求永久保留。

---

## 数据清洗例举1：观测性数据


以**观察性研究**中数据的清洗为例：

- 观察性数据有一个特点就是差异性，对同一个场景、同一个事件，不同的人去观察，看到的并非完全一致。 

- 每个人的观察记录，都有自己的习惯，有的习惯于采用速写和密写，比如说有些人为了防止别人看他的笔记，长采用密写的方式。 即使是结构式的观察，不同的观察者也会有特异性。 

- 观察性数据的清洗就需要把各类个性化的个人观察数据转变为标准化的观察记录。

---

## 数据清洗例举2：文献数据

以**文献数据**的清洗为例：

- 笔记的清洗。 比如说：研究用的素材如文献的阅读、标注与笔记、摘录，如果希望未来继续使用，那就需要**格式化清洗**，把素材转化为数据。如果有必要，还可以为下一步的数据库化做准备，比如编码。 

- 文献的清洗。对阅读过的文献，如果已经获得了数字版本，就需要与数字版本关联的编目信息、阅读信息关联起来整理，结合后边讨论的数据库化工作，把它们转化为个人档案馆。如果没有数字化的版本， 则需要将文献信息与阅读笔记信息关联，结合后边讨论的数据库化工作，把它们变成个人的档案阅读目录数据馆。 


---

## 数据清洗例举3：痕迹数据

以**痕迹数据**的清洗为例：

对痕迹数据的“四性”评估和清洗，一般是直接依据数据的来源来确认的。

比如，来自于**网络爬取**的数据，和来自于**数据拥有者机构**提供的数据，其它的**平行数据**等等。

一般而言，如果数据来源的渠道没有问题，数据的四性就不会有太大的问题。 

清洗痕迹数据最重要的一项工作，就是把**非格式化数据** 清洗为**格式化数据**（Why？至少目前的分析工具还不支持直接分析非格式化的数据）
 

那么格式化与结构化有什么区别呢？

**数据格式化**：把混杂在一堆数据中的各类数据清洗出来，分门别类。比如说日志数据中的用户行为数据，以淘宝数据为例，订单数据、发单数据、物流数据等等，分门别类整理出来。 

**数据结构化**：把各类数据和变量进行多维度关联。比如把以上日志数据中的各个子集关联到用户之下，形成类似于问卷调查数据的每个**样本数据**。 

---

## 数据清洗例举4：大数据

如果**痕迹数据**是**大数据**，情况就有些不同了。

在清洗数据之前，需要把**清洗策略**测试一遍，然后就可以直接采用大数据的清洗模式了。

- 从大数据中抽取数据，或者是从网页上爬取数据，在处理中尽管不一定会用到云计算， 在处理逻辑上还是一致的。 

- 大数据的清洗，目前运用比较普遍的是Hadoop框架下的Map Reduce。

---

## 数据清洗例举4：大数据（以阿里巴巴案例）


>阿里巴巴有淘宝、天猫、一淘等等业务，这些业务每时每刻都在产生数据，这些数据涉及到信用、金融、物流、管理等等业务操作。
所有这些操作的数据都会汇集到数据交换平台，由此构成了阿里巴巴的数据动态。

>2014年的双十一期间，6个小时之内的处理量就已经达到了100个PB。在产生的这些数据中，既有结构化的数据，也有非结构化的数据，进出数据平台的数据不是个，不是匹，而是流。这些数据流，通过数据处理就变成了中间层的数据，可以运用和应用于服务，中间服务，既可以对内，又可以对外。 

---

## 数据清洗例举4：大数据（以阿里巴巴案例）

问题是，这些数据是怎么处理的呢？数据清洗关心的正是[这个问题](https://www.alibabacloud.com/help/zh/doc-detail/27875.htm)。

```{r}
include_graphics("pic/chpt02-alibaba-map-reduce.jpg",dpi = 80)
```


???

我们来看看同学们初次听到这个概念可能会有些晕，没关系，我们这样理解， Hadoop相当于云计算，或者是分布式并行计算的操作系统，类似于个人电脑的Windows，或者是OS X。

这个操作系统下，有自己的文件系统，类似于Windows上有资源管理器，OS X下有Finder。也有它自己的数据库，比如，Hbase，类似于Windows上的数据库，比如微软的Access。 

不过呢，不管是文件系统还是数据库，与个人电脑最大的不同就在于它不是在一台电脑上，而是在成百上千台电脑上组成的分布式网络上，就像是一个军团。 对我们的数据清洗而言，实时记录的存储的大数据，通过这个框架系统，可以为清洗提供计算接口。其中，Map Reduce就是清洗数据的一个应用。

第一步，map，分布式的分类， 

第二步，reduce，就是分布式的合并同类项。

经过这两个 步骤，原来混杂的非结构化、结构化的数据变得结构化了。 其实呢，在map和reduce之间还有一个步骤，叫shuffling， 

就是通过交换位置的方式归类。我们来看例子，数据平台的 数据流，是什么数据都有的，这幅图呢，从左到右， 平台的数据就是Map Reduce的输入。 

第一步，通过分布式系统在云里分派任务， 

第二步，mapping，就是分类， 

第三步呢，通过交换位置进行归类，

第四步呢，reducing，就是把已经归类好的合并同类项，由此获得的产出是格式化的、结构化的数据， 可以用于分析与研究的数据库化的数据了。 这就是大数据的清洗，这些步骤都是在系统中完成的。 

---
layout: false
class: inverse, center, middle, duke-softblue

# 2.5 数据质量

---
layout: true
  
<div class="my-header"></div>

<div class="my-footer"><span>huhuaping@   第02章 数据收集、整理和清洗   
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;
2.5 数据质量</span></div> 

---


数据不仅整理好了，也清理好了，是不是就可以分析研究了呢？  还不行，调查数据的分析与研究，从计算机应用普及以来，就已经主要依靠计算机了。  依然采用手工计算的情形几乎已经消失了。  运用计算机就需要满足计算机对数据的要求，那就是数据库。  我们清理整理好的数据，要变成计算机可以读取 并进行运算的数据格式，通常这一类的格式  都是数据库格式。计算机应用程序不同对数据库的格式要求也不相同。  这一节我们讨论的就是常用的运用于 计算机单机统计计算与分析用的数据库化。  大数据的数据库化还有不一样的需求， 调查数据的数据库化，就是把变量、  变量属性或者标签输入计算机，变成结构化的数据矩阵。  在这个数字矩阵中，包括了变量名、变量属性标签 以及每一个调查样本对应不同变量的变量数据，  不管是哪一类的调查数据，数据库化是分析利用的前提，原因很简单，  数据的数量与复杂程度，已经超出了人们运用大脑、纸和笔，  直接处理的程度。运用计算机是最有效和最快捷的方式。  当然除了大数据以外，不同类型的调查数据，可以 数据库化的程度也不一样，方法也不一定相同。  还有，这里讨论的数据库化不是计算机网络系统的数据库化，  那是存储数据用的，有各种类型的数据库应用程序，常见的结构化数据库  SQL数据库有有多种，比如开源的免费的My Circle。  我们讨论的数据库化，是为了分析计算用的数据库化。  通过建立数据库，用于统计分析软件的计算，  


我们先从一个典型的统计分析软件的数据库表单开始。  SPSS数据库表单视图，  SPSS是社会科学统计计算运用比较多的一个大型统计计算软件，有单机版、  PC版、Mac都有，也有运行于服务器的版本，Linux和Unix也都有。  在这幅图中，我们看到除了常见的功能图标以外，先看最下面有两个按钮。  左边的按钮叫数据视图，右面的按钮叫变量视图，实际上  是两个表单的切换按钮，现在看到的是数据视图，  大家看表单第一行，是一个一个的变量，表的第一列是  调查样本的序号，一个样本的所有变量数据占据一行，  同样还是SPSS的数据表单，我们切换一下按钮，到变量视图， 就是对每一个变量的描述了，  第一行就是变量的各种属性，第一列就是变量的顺序排列，  从前面讨论的调查方法，我们已经知道，调查数据 有多种类型，我们先看问卷调查数据的数据库化。  问卷调查的数据，在完成了问卷的审核、归档、清理以后，  在用于分析软件的分析之前，就需要把它转化为  数据表示的数据库。怎么做呢？

通常有三个步骤：第一个步骤编码，在  第一阶段清理工作中，这项工作应该已经完成了，不过在数据入库之前 还需要审核。

第二个步骤，  数据录入与转化，如果是纸版问卷调查，这个时候就需要  录入数据。如果是计算机辅助调查，这个时候就需要转化数据。  如果需要录入，这需要看数据量有多大，  建议大家采用专门的录入软件进行录入，尽量避免录入中出现的差错，进而降低  调查误差。同样，如果需要做数据转化， 无论是内容转化还是格式转化，也建议尽量采用可靠的工具，  避免出现差错。  

第三个步骤，对录入完成和转化完成的数据，做  基本的检验和清理，最容易出现的差错就是错行、错列 造成数据的混乱。  在讨论问卷设计数据整理的时候， 我们已经提到了编码，不过没有详细讨论，现在我们需要详细讨论了。  编码就是把调查问卷的每一道访题 用符号或者数字组合代码换，  包括对每一道访题的选项或应答赋值。  这里每一道访题的编码就是数据库表中的变量。  应答赋值就是数据库表中的变量值，这个只有各种 属性，就是数据库表变量视图中的  各种标签，又称为变量标签。我们来看例子，这是Self PS中的一道访题，  

1.5，请问您希望孩子念书，最高念到哪一个程度？  选项有小学、初中、高中，然后专科、职高、  技校、大专、大学本科、本科以上、不必念书，共７个选项。  对这套访题我们可以这样编码，访题可以编为B15，  为什么这么编？依据的是问卷结构，当然你可以自己随便编， 不过最好还是有规律，有规律用起来就容易。  基于同样的道理，对选项的编码，就可以选项的编号。  用这个例子我们可以举一反三。  问卷调查数据的编码，一般有三种 方法：第一原始编码，就是直接运用问卷的编码。  通常这种方法仅仅用在访题数量极少，  应答非常简单的情况下。第二选编码，在调查开始之前，编码工作就已经做好了。  通常这种方法会用在基本上都是封闭访题的情况下。  只要是有开放访题，一般都会采用第三种方法，那就是后编码。  就是在问卷调查完成以后再做编码。  无论是采用哪种编码方法，最后都有一项相同的工作，就是编制编码部。  编码部相当于问卷数据的一个索引，把变量、变量值，  变量标签关联起来，类似于一本问卷数据字典。  

理解了上面这些事，就可以运用问卷数据了。  如果是自己调查，还需要学会一些操作活动，其中数据的录入是一项基础性的操作活动。  如果是简单的问卷调查，就可以运用常用的 办公软件来做录入，比如MS  Office 的Excel 就是最常见的数据库表，也是一个简易的统计分析工具。  类似的还有Mac上的Numbers，和Excel一样 既是一个数据库表，也是一个简易的统计分析工具。  还可以使用常见的统计分析软件的数据库表，比如SPSS的数据库表，  几乎所有的统计分析软件都有数据库表，比如Stata、Statistica 、R…等等。  除了使用通用的软件以外，稍微复杂一点的问卷调查最好  使用专门的数据录入软件，大型的统计分析软件一般都有专门用于  数据录入的软件或者模块，比如SPSS就有专门的模块叫做 Data  Entry，不过事情总是在变化中的，今天有没准不赚钱了，明天就没有了；  大家有预期就好。也可以使用免费的专用软件，比如EpiData  有中文界面，这是脱胎于美国NIH的流行病统计软件EPI的一个软件。  有人把其中的数据录入部分抽取了出来，变成 了免费的数据录入软件，有PC版，Mac版。  既可以做单机版，也可以做服务器版，网络地址在这，大家可以去下载来试试。  利用专用软件的好处是，可以把 纸版问卷计算机界面化，把纸版问卷完整的  呈现在计算机的屏幕上。不仅如此，还可以通过对 跳转、阈值、变量类型等的控制，尽量减少  录入所带来的误差。在录入完成以后，还可以直接把录好的数据导出为数据库表文件。  把文件的数据做一遍核查，就可以应用于分析研究了。  Epidata有多给平台的版本，这里是Windows版本的启动界面，通过  简单的编程，就可以进行数据录入了，具体的使用方法 可以参见软件的帮助文件，非常详细。  数据录入就是要把纸版问卷上的访题与应答转化为数据。  如果转化的过程中，稍有不慎，也会产生误差。为了控制 误差，一般而言会采用录入策略进行录入，  双录入是采用同样的工具，在完整地录完第一遍  以后，由同一个录入员，或者换一个录入员采用相同的方法录第二遍。  目的是通过第二遍的录入校验第一遍录入的准确性。  一般的录入工具都有校验功能，如果设定为双录入，在第二遍录入时 如果同一个变量的变量值出现不同，  软件就会跳出一个窗口，让录入员进行确认。这样就可以提醒  录入员认真地核对原始问卷上的数据，保证数据录入的准确性。  这样如果有并行数据，还可以把录入员的行为记录下来。  这样就可以研究录入行为，为改进录入工作提供依据，  录入完成的数据，就可以用于分析研究之前再一次数据清理工作了。  这次的清理针对的是已经数据库化的数据。  通常直接运用统计分析方法， 

第一清理录入错误。可以把双录入的数据输出为一个清理数据库，  核对录入中出现的冲突数据。  

第二编码清理，对不在编码值范围的变量值进行清理。  假设性别属性值的编码原本只有0和1，如果在数据表中出现了其它值，  那就一定是哪里有错误了，就需要清理并且改正错误。  

第三逻辑清理，主要是针对基本事实逻辑的清理。  比如样本为男性，在是否怀孕的访题下，变量值说明他有怀孕记录，这  就是逻辑错误，通过事实逻辑  判断变量值是否存在不符合逻辑的情况，如果有，那就需要清理并且改正错误。  改正的基本方法，就是核对原始文件。  在问卷数据的数据库化以后， 运用统计分析方法进行数据清理，上面的清理工作只是第一步。  在完成初步清理以后，还有进一步的工作要做。

比如离群值， 实际上可能是有效值的一部分，男性怀孕  令人奇怪，女性怀孕就没有什么让人奇怪的了，对不对？ 16岁－49岁之间怀孕都是正常的。  如果数据显示有一位七十岁的老奶奶怀孕了，  有没有可能呢？完全有可能，这个值不一定属于错误值。  属于有可能的值，不过不常见。所以被 称之为离群值，偏离了日常理解的范围。  

还有比如收入变量中的极大值和极小值， 都是需要再次确认的变量值。再比如无应答的处理，  通过分析已经应答的数值，确定对无应答的处理方式，比如差值  还有变量的再编码，比如受教育程度或者年龄的重新分组， 当然在数据的清理中也可以产生衍生变量，比如  依据受教育程度和收入来建构社会经济地位。  完成了这些工作，问卷数据的数据库化工作就基本完成了。  

正常的完成了数据库化的问卷数据，至少应该包括以下的文件：  

第一调查问卷，我们已经有了，也很容易。

第二调查问卷的数据库编码手册，也已经有了。  

第三两个数据库，一个是完问卷的数据库，一个是未完成问卷的数据库。  

第四样本数据库，通常抽样完成以后，一定有一个数据库，  这个数据库包括了用于抽样的变量，抽样单位，分层变量，权重变量等，  这些应该是分析研究之前已经有的数据。  

第五抽样报告、实施报告，这两份报告用于判断数据质量，制订分析策略。  

第六完成的、未完成问卷数量的统计表。  通常会把完成的、未完成的用表格方式展示出来， 

第七数据清理报告，对变量的可分析性要进行说明。  到这里为止，问卷数据就算是准备妥当了。  问卷数据的数据库化，可以被看作是结构化 数据的数据库化的标准，其它调查数据的数据库化  都是在这个基础上的增减，这一部分的内容还没有结束，我们先休息一会儿。

没有质量的数据，就是垃圾。本周探讨调查数据的整理和数据质量的评估。主要内容包括：调查数据有哪些类型？如何整理、清理并数据库化各类调查数据？数据的质量会受哪些因素的影响？如何评估数据质量？

---

```{r}
database2 <- read_table("data-raw/qiu-database.txt", col_names =F,
           locale = locale(encoding = stringi::stri_enc_get())) %>%
  as_tibble()  %>%
  mutate(X2 =str_extract_all(X1,"(?<=text\":\")(.*)(?=\")"))

database2 %>% extract2("X2") %>% str_c(collapse = "")
```

---

[1] "接下来我们看看访谈数据的数据库化。对访谈调查的数据，在完成了访谈笔记的整理、格式化、归档、清理之后，在用于分析之前也需要把相关的信息录到数据库中。虽然不一定可以像问卷调查数据那样完全的数据库化， 至少访谈记录与整理信息应该数据库化。访谈调查数据的数据库化同样有三个步骤。第一步，编码，与问卷编码不一样的是，访谈数据的编码有两类编码，一类记录信息的编码，另一类记录内容的编码，其中重点是第一类。对于第二类，除非要做文本分析，通常第二类编码可以不做。第二步，录入，在大多数情况下，会录入访谈记录信息，便于检索，也便于查找。如果要做内容分析，访谈内容就需要全部地录入，其实，在数字化存储的今天，最好都是全部录入，这样即使转换，分析工具 也很容易、也很便利。第三步，清理，在完成数据库化工作以后，还需要核查数据信息。与问卷调查数据的核查不同， 问卷调查数据的核查可以采用统计分析方法，访谈调查数据的核查除了记录信息数据以外，内容数据是没有办法采用统计分析方法的，需要逐行核查。对访谈数据的数据库化有了基本的了解之后，我们来看访谈数据的编码。访谈数据的编码有两类，一类是访谈记录信息的数据化，基本变量有记录编号、访谈时间、地点、人物、主题、位置图。如果有日志信息，也需要把日志信息加入其中。对访谈记录的编码，如果希望编码的程度可以 直接应用到内容分析软件的分析，那么就需要学习专门的课程，不同的分析软件对编码的要求是不一样的。即使不是这样，至少对访谈记录的内容也需要数字化，如果有数据表格，则数据表格也需要数据化。与问卷调查数据的数据库化比较，访谈调查数据的数据库化在工具上，并没有什么特别之处。数据录入通常也会使用办公软件， 只要是涉及到数字数据的，就可以使用Excel，对文本数据呢，就可以使用Word，当然也可以使用Numbers和Pages。如果有数字数据，也可以使用统计软件的数据表，比如SPSS的数据表，其他综合统计软件的数据表，再比如Stata，Statistica,r等等。此外，对访谈内容的数据库化还可以采用内容分析软件，比如Nvivo、Aquad、ATLAS.ti和Qualrus， 这些软件不是常用的工具，不像Excel和Word那样常用，大家如果希望用来做研究， 就需要专门的学习。同样对访谈数据的录入也有策略问题，对于访谈记录信息的录入，尽量采用标准化的格式，目的是便于交换、便于交流。对访谈记录内容的录入，可以先转录为 纯文本格式，注意纯文本格式有一个编码问题，最好采用通用的编码，比如Unicode。有了纯文本，就可以运用与其他文本之间的交换， 对转录完成的文本，先做内容校对，待确认无误以后，就可以再编码，并且导入分析工具。和问卷调查数据的数据库化一样，一份完整的访谈数据的数据库化产出也有一份清单，至少要有以下的数据文件，比如调查提纲或者访谈提纲，或者访谈设计。访谈记录的 整理、清理的数据库，访谈内容的数据库，访谈记录的数字化，也就是数字化的过程及报告，最后还有清理报告。这就是访谈数据的数据库化了。现在我们来看观察数据的数据库化。观察数据怎么数据库化呢？其实和访谈数据差不多，事儿比较多，在完成观察记录的格式化之后，就要想办法把它录到数据表格里边去了。也是三个步骤：第一步，编码， 观察调查数据的编码与其他编码不一样的地方在于观察记录信息比访谈记录信息要丰富得多。当然对观察记录的内容，如果希望用作分析素材，也需要编码。第二步，录入，在大多数情况下，主要录入观察记录信息，同样，如果要把观察记录的内容作为统计分析的素材，那么也需要把它录到数据库中。第三步，清理，同样在录入完成之后，要对已经录入的数据进行核查，如果有观察记录的内容，就需要对已经数据库化的内容做仔细的核查，确保内容准确。只要是数据库化，就离不开编码， 观察数据也不例外。要编码的观察记录信息基本变量包括记录编号、观察的时间、地点、事件、主题，还有观察媒体，比如说是拿望远镜还是拿摄像机，还是用自己的眼睛。如果有日志信息，也可以把日志信息列入其中。对观察记录内容的编码和访谈记录内容的编码一样， 也是一项复杂的工作，需要专门学习，特别是根据分析工具来学习， 即使观察记录的内容不会作为统计分析的素材，最好还是录入为数据化的文本文件，便于交流。观察记录的录入， 其中的文本数据、数字数据和访谈调查的数据库化一样。如果是图片数据呢，则可以采用类似于Adobe的Lightroom之类的数据库； 如果是视频数据呢，则可以运用类似于Adobe Premier之类的编辑库；同样，音频数据也可以寻找 适用的音频数据库。观察数据也有录入的策略问题，比较而言，观察数据的录入策略更接近于访谈数据的录入策略，那就是不管是哪一类的数据，尽量采用标准化的数据格式， 除了便于交换，还便于未来使用和更新。在具体录入方法上，对文本记录先转录为通用的文本格式，比如word啊，pages啊，或者就是纯文本啊，或者是副文本呀，便于其他的文本工具使用。对图片记录，可以先扫描，再录入记录信息，如果已经是电子化的文档，就直接录入，在录入的同时呢，也录入记录信息。所有这些数据在初次录入以后一定要校对，校对之后 就可以进行后续处理，比如再编码呀，或者导入分析工具了。一份完整的数据库化的观察数据的数据库， 至少要提供以下的数据文件。观察提纲或者观察设计；观察记录的整理、清理数据库；观察内容数据库； 观察记录数据数字化、数据库化过程的数据；清理报告。在调查数据中还有一类文献数据，和其他调查数据一样，文献数据也需要数据库化。与其他调查数据有些不同的是，文献调查的数据一般情况下原本就来源于数据库，无论是数字化的文献，还是非数字化的文献，原本就表现为数据库的形式。因此，运用原来数据库的数据，是文献调查数据库的特点。即使原来的数据库不是数字化的数据库，也没有关系，数据的格式化、结构化的特征是完整的。因此呢，文献数据的数据库化的三个步骤 与其他数据的虽然一致，但内容有些差别。编码，指的是文献信息的编码，而不是文献内容的编码，文献信息就是编目信息，文献内容就是文献记载的内容。录入呢，指的是过录，就是把原来数据库的 文献编目信息和文献内容抄录到研究用的文献数据库中去。清理，就是在数据录入 完成以后，对录入的数据进行核查、清理，包括完整性检查。为了确保同学们已经掌握了文献编目信息，我重复一遍文献的编码。文献数据的记录信息的基本变量主要有作者、篇名、时间、载体、存放、DOI，或者ISBO，或者ISNN等。文献记录的编码可以直接运用文献记录的原始编码，一些数据库化的数据，比如jasdo，还支持编码的数据直接导出。在文献调查数字领域，也有专门的 数据软件，也是个人化的文献管理软件。第一个是用于单机的， 既有PC版，也有mac版，Endnote原本这个软件只是用于参考文献管理 与Ms Office的文字处理工具配合，方便客人用于写作。经过20多年的发展，这个软件目前已经成为了众多同类软件中存在时间最长久的一个，既可以作为文献调查的工具，也可以作为文献存储的工具。当然，其最基本的功能——文献引用功能还在， 也是文献引用的工具。我建议同学们从文献调查开始就学会使用这种软件，对文献数据的调查、数据库化管理引用 都非常有帮助。Endnote，是最先有PC版，后来才有mac版。我要介绍的第二个工具软件，Papers，正好相反，是先有mac版， 后有pc版的。Papers是最近几年从mac上兴起的一个工具，与Endnote不同，它一开始就是一个数据库化了的工具，集成了多个数据库接口， 可以通过互联网络直接连接到不同的数据库去查询数据。世界主要的图书馆，大学图书馆都有接口。当然，可以作为文献调查的工具，可以作为 文献数据库的工具，也可以作为文献研读的工具，还可以作为文献引用的工具。最后我们要简要地讨论痕迹数据的数据库化。大家还记得我们讨论过的Map_Reduce？Map_Reduce的产出相当于问- 卷调查中计算机辅助调查的问卷数据,不过呢，与计算机辅助调查的问卷数据不同的是，计算机辅助调查的数据，是样本化的数据，也就是 每一个变量的数据，是与样本关联在一起的；而痕迹数据，无论是Map-Reduce的产出，还是网页爬取 的数据的整理、清理时的产出，都是基于变量的数据，还没有把变量数据串起来，变成基于样本的数据。我们知道，样本在变量上的变异是分析工作的 基础，因此呢，数据库化需要做的工作就是把变量数据串起来，变成类似于样本数据的数据。串起来的方法很多，技术性也很强，基本上依靠脚本来完成。如果是对全数据的分析，仍然需要基于分布式并行计算来完成， 这就是大数据的数据库化问题，超出了我们课程范围。如果从大数据中抽取数据， 由于无需数据录入，故数据库化只有两个步骤可做。编码，通常原有的数据就已经有编码了，这个手续要做的就是要么确认使用原来的编码，要么呢，因为特殊的原因，需要重新编码，何去何从，完全取决于计算的需要。清理，与其他调查数据的清理不同，这里主要是在确认编码以后，确认数据的可计算性，也就是 格式化、结构化在转化中没有发生问题，以及是否可以直接运用于分布式并行计算或者单机计算。下边我们把这一节的内容做一个小结。数据库化是调查数据整理与清理之后的一个产出，也是在进入数据分析之前的最后阶段。不同调查数据的数据库化，有不同的方式跟方法，基本的要求是通过数据库化，让调查数据格式化、结构化，符合统计分析、计算对数据的要求，因此呢，数据库化的目的就是为了便于分析和使用。需要提醒的是痕迹数据，如果是大数据，尤其是对全数据或者数据流的分析，则需要采用适合分布式并行计算的数据库化方案。"



---
layout:false
background-image: url("pic/thank-you-gif-funny-little-yellow.gif")
class: inverse,center
# 本章结束
